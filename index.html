<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>InforMARL</title>

    <meta name="description" content="InforMARL">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--FACEBOOK-->
    <!-- <meta property="og:image" content="img/twitter-card.jpg"> -->
    <!-- <meta property="og:image:type" content="image/png"> -->
    <!-- <meta property="og:image:width" content="1024"> -->
    <!-- <meta property="og:image:height" content="512"> -->
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://nsidn98.github.io/InforMARL/" />
    <meta property="og:title" content="InforMARL" />
    <meta property="og:description"
        content="Project page for InforMARL: Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="https://nsidn98.github.io/InforMARL/" />
    <meta name="twitter:title" content="InforMARL" />
    <meta name="twitter:description"
        content="Project page for InforMARL: Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation." />



    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-5H2C4DFSMD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-5H2C4DFSMD');
    </script> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                InforMARL: Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation. </br>
                <small>
                    ICML 2023
                </small>
            </h1>

        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://nsidn98.github.io">
                            Siddharth Nayak
                        </a>
                        </br>MIT
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/kennethschoi/">
                            Kenneth Choi
                        </a>
                        </br>MIT
                    </li>
                    <li>
                        <a href="https://github.com/dingwq22">
                            Wenqi Ding
                        </a>
                        </br>MIT
                    </li>
                <!-- </br> -->
                    <li>
                        <a href="https://sydneyidolan.com/">
                            Sydney Dolan
                        </a>
                        </br>MIT
                    </li>
                    <li>
                        <a href="https://karthikg92.github.io/">
                            Karthik Gopalakrishnan
                        </a>
                        </br>Stanford University
                    </li>
                    <li>
                        <a href="https://aeroastro.mit.edu/dinamo-group/">
                            Hamsa Balakrishnan
                        </a>
                        </br>MIT
                    </li>
                </ul>

                <!-- *denotes equal contribution -->
            </div>
        </div>


        <div class="row">
            <!-- <div class="col-md-5 col-md-offset-3 text-center"> -->
            <div class="col-md-4 col-md-offset-3 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2211.02127">
                            <image src="img/nice_paper_png.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/nsidn98/InforMARL">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.dropbox.com/s/y45cujak4mvaefo/InforMARL%20%28Dec%29%20Slides.pptx?dl=0">
                            <image src="img/ppt.png" height="60px">
                                <h4><strong>Slides</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="https://www.youtube.com/watch?v=ooj9E-endGc">
                            <image src="img/yt.png" height="60px">
                                <h4><strong>YouTube</strong></h4>
                        </a>
                    </li> -->
                    
                    <li>
                        <a href="http://nsidn98.github.io/files/Publications_assets/InforMARL/poster/ICML_InforMARL_Poster.pdf">
                            <image src="img/ICML_InforMARL_Poster.png" height="60px">
                                <h4><strong>Poster</strong></h4>
                        </a>
                    </li>

                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                    <p class="text-justify">
                        We consider the problem of multi-agent navigation and 
                        collision avoidance when observations are limited to the 
                        local neighborhood of each agent. We propose <b>InforMARL</b>, 
                        a novel architecture for multi-agent reinforcement learning (MARL) 
                        which uses local information intelligently to compute paths 
                        for all the agents in a decentralized manner. Specifically, 
                        InforMARL aggregates information about the local neighborhood 
                        of agents for both the actor and the critic using a graph 
                        neural network and can be used in conjunction with any 
                        standard MARL algorithm. We show that (1) in training, 
                        InforMARL has better sample efficiency and performance 
                        than baseline approaches, despite using less information,
                        and (2) in testing, it scales well to environments
                        with arbitrary numbers of agents and obstacles.
                        We illustrate these results using four task environments, 
                        including one with predetermined goals
                        for each agent, and one in which the agents collectively 
                        try to cover all goals.

                        <!-- We consider the problem of multi-agent navigation and 
                        collision avoidance when observations are limited to the 
                        local neighborhood of each agent. We propose <b>InforMARL</b>, 
                        a novel architecture for multi-agent reinforcement learning (MARL) 
                        which uses local information intelligently to compute paths 
                        for all the agents in a decentralized manner. Specifically, 
                        InforMARL aggregates information about the local neighborhood 
                        of agents for both the actor and the critic using a graph 
                        neural network and can be used in conjunction with any 
                        standard MARL algorithm. We show that (1) in training, 
                        InforMARL has better sample efficiency and performance 
                        than baseline approaches, despite using less information, 
                        and (2) in testing, it scales well to environments with 
                        arbitrary numbers of agents and obstacles. -->
                        <br>
                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation
                </h3>
                <!-- <image src="img/fig_1.jpg" class="img-responsive" alt="overview"><br> -->
                    <p class="text-justify">
                        MARL-based techniques have achieved significant successes 
                        in recent times, eg., DeepMind's AlphaStar surpassing 
                        professional level players in <cite><a href="https://www.nature.com/articles/s41586-019-1724-z">StarCraft II</a></cite>, 
                        OpenAI Five defeating the world-champion in <cite><a href="https://arxiv.org/abs/1912.06680">Dota II</a></cite>, etc. 
                        The performance of many of these MARL algorithms depends 
                        on the amount of information included in the state given 
                        as input to the neural networks <cite><a href="https://openreview.net/forum?id=YVXaxB6L2Pl">[1]</a></cite>.
                        In many practical multi-agent scenarios, each agent aims 
                        to share as little information as possible to accomplish 
                        the task at hand. This structure naturally arises in many 
                        multi-agent navigation settings, where agents may have a 
                        desired end goal but do not want to share their information 
                        due to communication constraints or proprietary concerns 
                        <cite><a href="https://ieeexplore.ieee.org/document/7208459">[2], </a></cite>
                        <cite><a href="https://www.fcc.gov/ecfs/search/search-filings/filing/1040578949828">[3]</a></cite>. 
                        These scenarios result in a decentralized structure, 
                        as agents only have locally available information about 
                        the overall system's state. In this paper, we focus on 
                        the question: <i><b>"Can we train scalable multi-agent 
                        reinforcement learning policies that use limited local 
                        information about the environment to perform collision-free 
                        navigation effectively?" </b></i>

                        <br><br>

                        <h4>
                            A Motivating Experiment
                        </h4>
                        The amount of information available to each agent determines 
                        whether or not the agent can learn a meaningful policy.
                        Although having more information is generally correlated 
                        to better performance, it does not necessarily scale well 
                        with the number of agents. Prior works like MAPPO <cite><a href="https://openreview.net/forum?id=YVXaxB6L2Pl">[1] </a></cite>, MADDPG
                        <cite><a href="https://arxiv.org/abs/1706.02275">[4] </a></cite>
                        have typically used a na&iumlve concatenation of the states 
                        of all agents or entities in the environment fed into a 
                        neural network. Such an approach scales poorly 
                        (the network input size is determined by the number of agents) 
                        and does not transfer well to scenarios with a different 
                        number of agents than the training environment. We illustrate 
                        the dependence of the learned policies on the amount of 
                        information available to agents by defining three information modes: 
                          <ul>
                            <li>
                                <b>Local</b>: In the local information mode, 
                                <img src="https://latex.codecogs.com/svg.image?o^{(i)}_{\mathrm{loc}}&space;=&space;[p^{(i)},&space;v^{(i)},&space;p^{(i)}_{\mathrm{goal}}]"/>
                                where <img src="https://latex.codecogs.com/svg.image?p^{(i)}"/> 
                                and <img src="https://latex.codecogs.com/svg.image?v^{(i)}"/> 
                                are the position and velocity of agent 
                                <img src="https://latex.codecogs.com/svg.image?{i}"/> 
                                in a global frame, and <img src="https://latex.codecogs.com/svg.image?p^{(i)}_{\mathrm{goal}}"/> 
                                is the position of the goal relative to the agent's position.
                            </li>
                            <li>
                                <b>Global</b>: Here, 
                                <img src="https://latex.codecogs.com/svg.image?o_{\mathrm{glob}}^{(i)}&space;=&space;[p^{(i)},&space;v^{(i)},&space;p^{(i)}_{\mathrm{goal}},&space;p^{(i)}_{\mathrm{other}}]" />
                                where <img src="https://latex.codecogs.com/svg.image?p^{(i)}_{\mathrm{other}}"/> 
                                comprises of the relative positions of all 
                                the other entities in the environment. The 
                                scenarios defined in the MAPE 
                                (and consequently, other approaches that use MAPE) 
                                use this type of information mode unless explicitly stated otherwise.
                            </li>
                            <li>
                                <b>Neighbourhood</b>: In this information mode, 
                                agent <img src="https://latex.codecogs.com/svg.image?{i}"/> 
                                observes <img src="https://latex.codecogs.com/svg.image?o_{\mathrm{nbd}}^{(i)}&space;=&space;[p^{(i)},&space;v^{(i)},&space;p^{(i)}_{\mathrm{goal}},&space;p^{(i)}_{\mathrm{other}}]"/>
                                where <img src="https://latex.codecogs.com/svg.image?p^{(i)}_{other}"/> 
                                comprises of the relative positions of all other 
                                entities which are within a distance <i>nbd-dist</i> 
                                of the agent. The maximum number of entities 
                                within the neighborhood is denoted <i>max-nbd-entities</i>, 
                                and so the dimension of the observation vector is fixed. 
                                If there are fewer than <i>max-nbd-entities</i> 
                                within a distance <i>nbd-dist</i> of the agent, 
                                we pad this vector with zeros.
                            </li>
                          </ul>
                        <!-- where we maximize the number of pilots getting assigned 
                        to slots in flights. Here ð¼ is the set of pilots, ð‘† is 
                        the set of slots in flights. -->
                        <br>
                        <br>

                        <center>
                            <figure>
                                <image src="img/plots/local_nbd_global.png" class="center" alt="The MDP for RL agent" height="340"/>
                                <figcaption><b><br>Figure 1:</b> MAPPO with the 
                                    local, neighborhood (with 1, 3, and 5 
                                    <i>max-nbd-entities</i>), and global 
                                    information given as states. The plots show 
                                    the rewards during training for the 3 agent-3 
                                    obstacle navigation scenario. The means and 
                                    standard deviations of the rewards over training 
                                    with five random seeds are shown. Comparing the 
                                    global information mode to the others, we see 
                                    that merely providing local information and 
                                    a na&iumlve concatenation of neighborhood 
                                    information is not sufficient to learn an 
                                    optimal policy. </figcaption>
                            </figure>
                        </center>


                        <br>
                        <br>
                        <center>
                            <figure>
                                <image src="img/less_info.png" class="center" alt="The MDP for RL agent" height="340"/>
                                <figcaption><b><br>Figure 2 </b> The title of our paper in an alternate universe where academic papers can be informa(r)l </figcaption>
                            </figure>
                        </center>

                        <!-- <br>
                        <br> -->

                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <p class="text-justify">
                        This is where our MARL framework for navigation - <i>InforMARL</i> comes into the picture. InforMARL, consists 
                        of four modules, as shown in Figure 1. We describe each in detail below:
                    <br>


                    <center>
                        <figure>
                            <image src="img/graphMARLArch.v8.png" class="center" alt="The MDP for RL agent" height="340"/>
                            <figcaption><b><br>Figure 3:</b> Overview of our method - <i>InforMARL</i> </figcaption>
                        </figure>
                    </center>

                    
                    <br>
                    (i) <u><b>Environment</b></u>: The agents are depicted by green circles, 
                    the goals are depicted by red rectangles, and the unknown 
                    obstacles are depicted by gray circles. 
                    <img src="https://latex.codecogs.com/svg.image?x^{(i)}_{\mathrm{agg}}"/> represents the aggregated information from the 
                    neighborhood, which is the output of a graph neural network. A graph is created 
                    by connecting entities within the sensing-radius of the agents. 
                    <br>
                    <br>
                    (ii)  <u><b>Information Aggregation</b></u>: Each agent's observation 
                    <img src="https://latex.codecogs.com/svg.image?o^{(i)}"/> is 
                    concatenated with <img src="https://latex.codecogs.com/svg.image?x^{(i)}_{\mathrm{agg}}"/>. 
                    The inter-agent edges are bidirectional, while the edges 
                    between agents and non-agent entities are unidirectional. 
                    <br>
                    <br>
                    (iii) <u><b>Graph Information Aggregation</b></u>: The aggregated vector 
                    from all the agents is averaged to get <img src="https://latex.codecogs.com/svg.image?X_{\mathrm{agg}}"/>.
                    <br>
                    <br>
                    (iv) <u><b>Actor-Critic</b></u>: The concatenated vector 
                    <img src="https://latex.codecogs.com/svg.image?[o^{(i)}, x^{(i)}_{\mathrm{agg}}]"/> 
                    is fed into the actor network to get the action, and 
                    <img src="https://latex.codecogs.com/svg.image?X_{\mathrm{agg}}"/>
                    is fed into the critic network to get the state-action values.
                    
                    <br>
                    <br>
                    <br>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments and Results
                </h3>
                    <p class="text-justify">
                        <h4>
                            Environments
                        </h4>
                        We evaluate our method on the following environments:
                        <ul>
                            <li> <b>Target</b>: Each agent tries to reach its preassigned 
                                goal while avoiding collisions with other entities 
                                in the environment.</li>
                            <li> <b>Coverage</b>: Each agent tries to go to a goal 
                                while avoidin collsions with other entities, and 
                                ensuring that no more than one agent reaches 
                                the same goal.</li>
                            <li> <b>Formation</b>: There is a single landmark (the counterpart 
                                of a goal for this task) and the agents try to position
                                themselves in an N-sided regular polygon with the 
                                landmark at its centre.</li>
                            <li> <b> Line</b>: There are two landmarks, and the 
                                agents try to position themselves equally spread 
                                out in a line between the two.</li>
                        </ul>

                        <center>
                            <div class="row">
                                <div style="float:left; margin-right:20px;">
                                    <img src="img/navigation.png" height="150" width="150" style="border: 1px solid #555" />
                                    <p style="text-align:center;">Target</p>
                                </div>
                                <div style="float:left; margin-right:20px;">
                                    <img src="img/spread.png" height="150" width="150" style="border: 1px solid #555" />
                                    <p style="text-align:center;">Coverage</p>
                                </div>
                                <div style="float:left; margin-right:20px;">
                                    <img src="img/formation.png" height="150" width="150" style="border: 1px solid #555" />
                                    <p style="text-align:center;">Formation</p>
                                </div>
                                <div style="float:left; margin-right:20px;">
                                    <img src="img/line.png" height="150" width="150" style="border: 1px solid #555" />
                                    <p style="text-align:center;">Line Formation</p>
                                </div>
                            </div>
                            <figcaption><b><br>Figure 4</b>: The agents are shown in blue circles, the goals are shown in green and obstacles are shown in black in the Target
                                and Coverage environment. The landmarks are shown in black in the Formation and Line environments.</figcaption>
                        </center>
                        

                        
                        <!-- <center>
                            <div class="row">
                                <div class="column">
                                    <img src="img/navigation.png" style="float: left; height: 150px; margin-right: 2%; margin-bottom: 0.5em; border: 1px solid #555">
                                </div>
                                <div class="column">
                                    <img src="img/spread.png" style="float: left; height: 150px; margin-right: 2%; margin-bottom: 0.5em; border: 1px solid #555">
                                </div>
                                <div class="column">
                                    <img src="img/formation.png" style="float: left; height: 150px; margin-right: 2%; margin-bottom: 0.5em; border: 1px solid #555">
                                </div>
                                <div class="column">
                                    <img src="img/line.png" style="height: 150px; border: 1px solid #555">
                                </div>
                                <figcaption><b><br>Figure 4</b>: The <i>Target</i>, <i>Coverage</i>, <i>Formation</i> and <i>Line</i> environments. 
                                </figcaption>
                            </div>
                        </center> -->
                        <!-- <br> -->

                        <h4>
                            Comparisons against other methods
                        </h4>
                        We compare our method with a few different MARL baselines 
                        in the <i>Target</i> environment. 
                        <br>

                        <div class="row">
                            <div class="column">
                                <img src="img/plots/compare_3_short.png" style="float: left; width: 33%; margin-right: 0%; margin-bottom: 0.5em;">
                                <!-- <img src="img/plots/compare_3_short.png" alt="Snow" style="width:30%"> -->
                            </div>
                            <div class="column">
                                <img src="img/plots/compare_7_short.png" style="float: left; width: 33%; margin-right: 0%; margin-bottom: 0.5em;">
                                <!-- <img src="img/plots/compare_3_short.png" alt="Forest" style="width:30%"> -->
                            </div>
                            <div class="column">
                                <img src="img/plots/compare_10_short.png" alt="Mountains" style="width:33%">
                            </div>
                            <div class="column">
                                <img src="img/plots/legend_3_short_transpose.png" alt="Mountains" style="width:100%">
                            </div>
                            <figcaption><b><br>Figure 5</b>: Comparison of the 
                            training performance of InforMARL with the 
                            best-performing baselines using global and local information. 
                            InforMARL significantly outperforms most baseline algorithms. 
                            Although RMAPPO has similar performance, it requires global information. 
                            Refer to Appendix in the <a href="https://arxiv.org/abs/2211.02127">paper</a> 
                            for a complete comparison to more baselines. </figcaption>
                        </div>

                        <br>
                        <br>
                        The following metrics are compared:
                        <ul>
                            <li> Total reward obtained in an episode by all the agents (higher is better). </li>
                            <li> Fraction of episode taken by the agents to reach the goal, <img src="https://latex.codecogs.com/svg.image?\textbf{T}"/> (lower is better).</li>
                            <li> The total number of collisions the agents had in the episode, # col (lower is better).</li>
                            <li> Percent of episodes in which all agents are able to get to their goals, <img src="https://latex.codecogs.com/svg.image?S\%"/> (higher is better).</li>
                        </ul>

                        The best-performing methods that use global information (RMAPPO) and local information (InforMARL) are highlighted.

                        <figure>
                            <image src="img/compare_table.png" class="noPadding" alt="Comparison Table" height="350"/>
                            <figcaption><b><br>Table 1</b>: Comparison of InforMARL 
                            with other baseline methods, for scenarios with 3, 7, and 10 
                            agents in the <i>Target</i> environment. The results presented 
                            represent the average of 100 test episodes.  </figcaption>
                        </figure>

                        <br>

                        <h4>
                            Scalability of InforMARL
                        </h4>

                        <center>
                            <figure>
                                <image src="img/scale_table.png" class="center" alt="The weight extraction method" height="350"/>
                                <figcaption><b><br>Table 2</b>: Test performance of InforMARL for the <i>Target</i> environment, 
                                when trained on scenarios with 
                                <img src="https://latex.codecogs.com/svg.image?n"/> 
                                agents and tested on scenarios with 
                                <img src="https://latex.codecogs.com/svg.image?m"/> 
                                agents in the environment. 
                                <!-- InforMARL is able to achieve a 
                                success rate of nearly 100% while the fraction 
                                of episodes taken to reach the goals 
                                (<img src="https://latex.codecogs.com/svg.image?T"/>) 
                                is almost constant. Note that we normalize the 
                                reward and the number of collisions by 
                                <img src="https://latex.codecogs.com/svg.image?m"/> 
                                for ease of comparison. -->
                                </figcaption>
                            </figure>
                        </center>

                        <h4>
                            Performance in Other Task Environments
                        </h4>
                        <center>
                            <figure>
                                <image src="img/multi_envs.png" class="center" alt="other envs" height="250"/>
                                <figcaption><b><br>Table 3</b>: Performance of RMAPPO and InforMARL on the
                                    coverage, formation, and line tasks. We note that Infor-
                                    MARL was trained on the 3-agent scenario and tested on
                                    m = {3, 7} agents, while RMAPPO was trained and tested
                                    on the same number of agents (i.e., with m = n).
                                </figcaption>
                            </figure>
                        </center>
                        
                        <h4>
                            Effect of sensing radius
                        </h4>

                        <center>
                            <figure>
                                <image src="img/plots/compare_rad_v2.png" class="center" alt="The weight extraction method" height="400"/>
                                <figcaption><b><br>Figure 6</b>: Diminishing 
                                returns in performance gains from increasing the 
                                sensing radius for InforMARL. The dashed lines 
                                are the reward values after saturation for RMAPPO 
                                in the global (in green) and local (in red) 
                                information modes, respectively. They are 
                                provided for reference.
                                </figcaption>
                            </figure>
                        </center>

                        <h4>
                            Ablation study for Graph Information Aggregation Module
                        </h4>

                        <center>
                            <figure>
                                <image src="img/plots/cent_obs_ablation.png" class="center" alt="The weight extraction method" height="400"/>
                                <figcaption><b><br>Figure 7</b>: Training 
                                performance of InforMARL with, and without, the 
                                graph information aggregation module, for a 
                                3-agent scenario. The two variants have similar 
                                sample complexities. However, the critic network 
                                with the graph information aggregation module has 
                                fewer parameters than the one without this module.
                                </figcaption>
                            </figure>
                        </center>

                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Conclusions
                </h3>
                    <p class="text-justify">
                        <ul>
                            <li> 
                                We showed that having just local observations as 
                                states is not enough for standard MARL algorithms 
                                to learn meaningful policies. 
                            </li>

                            <li> 
                                Along with this, we also showed that 
                                albeit na&iumlvely concatenating state information 
                                about all the entities in the environment helps 
                                to learn good policies, they are not transferable 
                                to other scenarios with a different number of 
                                entities than what it was trained on.
                            </li>

                            <li> 
                                InforMARL is able to learn transferable policies 
                                using standard MARL algorithms using just local 
                                observations and an aggregated neighborhood 
                                information vector. Furthermore, it has better 
                                sample complexity than other standard MARL 
                                algorithms that use global observation.
                            </li>
                        </ul>
                        <br>
                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Future Work
                </h3>
                    <p class="text-justify">
                        <ul>
                            <li> 
                                Include the introduction of more complex 
                                (potentially adversarial) dynamic obstacles in 
                                the environment.
                            </li>

                            <li> 
                                Adding a safety guarantee layer for the actions 
                                of the agents to avoid collisions at all costs.
                            </li>

                            <li> 
                                Investigate the use of InforMARL for curriculum 
                                learning and transfer learning to more complex environments.
                            </li>
                        </ul>
                        <!-- <br> -->
                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    Video Presentation (Coming Soon)
                </h3> -->

                <!-- <br>
                <p align="center">
                <div class="embed-responsive embed-responsive-16by9">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/dQw4w9WgXcQ" 
                    title="YouTube video player" frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div> -->

                <!-- </p> -->

                <!-- <br> Presented at the Conference <a href="https://conference-website/">Main track</a>. -->
                <br>
                <table align=center width=800px>
                    <br>
                    <tr>
                        <center>
                            <span style="font-size:22px">&nbsp;<a
                                    href='https://www.dropbox.com/s/y45cujak4mvaefo/InforMARL%20%28Dec%29%20Slides.pptx?dl=0'>[Slides]</a>
                </table>


                <object width="800" height="500" type="application/pdf" data="img/slides.pdf?#zoom=60&scrollbar=0&toolbar=0&navpanes=0">
                    <p>Insert your error message here, if the PDF cannot be displayed.</p>
                </object>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h2>
                    Citation
                </h2>
                <br />If you find our work or code useful in your research, please consider citing the following:
                <br />
                <br />
                <div class="form-group col-md-12 col-md-offset-0">
                    <textarea id="bibtex" class="form-control" rows="13" readonly>
@article{nayak22informarl,
    doi = {10.48550/ARXIV.2211.02127},
    url = {https://arxiv.org/abs/2211.02127},
    author = {Nayak, Siddharth and Choi, Kenneth and Ding, Wenqi and Dolan, Sydney and Gopalakrishnan, Karthik and Balakrishnan, Hamsa},
    keywords = {Multiagent Systems (cs.MA), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International}
    }                        
                    </textarea>
                </div>
                <!-- <br /> -->
                <!-- <br /> -->
                <br />
                <div class="form-group col-md-12 col-md-offset-0">
                    <textarea id="bibtex" class="form-control" rows="16" readonly>
@InProceedings{pmlr-v211-dolan23a,
    title = 	 {Satellite Navigation  and Coordination with Limited Information Sharing},
    author =       {Dolan, Sydney and Nayak, Siddharth and Balakrishnan, Hamsa},
    booktitle = 	 {Proceedings of The 5th Annual Learning for Dynamics and Control Conference},
    pages = 	 {1058--1071},
    year = 	 {2023},
    editor = 	 {Matni, Nikolai and Morari, Manfred and Pappas, George J.},
    volume = 	 {211},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {15--16 Jun},
    publisher =    {PMLR},
    pdf = 	 {https://proceedings.mlr.press/v211/dolan23a/dolan23a.pdf},
    url = 	 {https://proceedings.mlr.press/v211/dolan23a.html},
    }
                    </textarea>
                </div>
                <br />
                <br />
                <!-- <div class="form-group col-md-12 col-md-offset-0">
                    <textarea id="bibtex" class="form-control" rows="6" readonly>
@mastersthesis{Nayak2022,
    author  = "Siddharth Nagar Nayak",
    title   = "Learning-based Scheduling",
    school  = "MIT",
    year    = "2022"
    }
                    </textarea>
                </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related Links
                </h3>
                <p class="text-justify">
                    <ul>

                        <li>
                            <a href="https://arxiv.org/abs/2211.03658">Application </a> 
                            of InforMARL for space traffic management with 
                            minimum information sharing.
                            <br />
                        </li>
                        
                        <!-- <li>
                            Reinforcement learning based methods for scheduling by <a href="https://dspace.mit.edu/handle/1721.1/145097">Siddharth Nayak
                                (2022)</a>.<br />
                        </li>

                        <li>
                            The buffer formulation was introduced by 
                            <a href="https://dspace.mit.edu/handle/1721.1/139538">Christopher Chin.
                            (2021)</a>, for the robust crew-scheduling problem.<br />
                        </li>

                        <li>
                            Integer programming formulation for different 
                            objective functions was introduced by <a href="https://hdl.handle.net/1721.1/139080">Matthew Koch 
                                (2021)</a>.<br />
                        </li>

                        <li>
                            <a href="https://vimeo.com/476994227/cd722a770a"> Talk</a> by Christopher Chin on AI-assisted Optimization 
                            of Schedules.<br />
                        </li> -->
                        <br />
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <br />The authors would like to thank the 
                    <a href="https://supercloud.mit.edu/">MIT SuperCloud</a> 
                    and the Lincoln Laboratory Supercomputing Center for providing 
                    high performance computing resources that have contributed to 
                    the research results reported within this paper. 
                    The NASA University Leadership initiative (grant #80NSSC20M0163) 
                    provided funds to assist the authors with their research, 
                    but this article solely reflects the opinions and conclusions 
                    of its authors and not any NASA entity. This research was 
                    sponsored in part by the United States AFRL and the United 
                    States Air Force Artificial Intelligence Accelerator and was 
                    accomplished under Cooperative Agreement Number FA8750-19-2-1000. 
                    The views and conclusions contained in this document are 
                    those of the authors and should not be interpreted as 
                    representing the official policies, either expressed or implied, 
                    of the United States Air Force or the U.S. Government. 
                    The U.S. Government is authorized to reproduce and distribute 
                    reprints for Government purposes notwithstanding any copyright 
                    notion herein. Sydney Dolan was supported by the National 
                    Science Foundation Graduate Research Fellowship under Grant No. 1650114.
                    <br />
                    <br /> This website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and
                    <a href="https://www.matthewtancik.com">Matthew Tannick</a>.
                </p>
            </div>
        </div>
    </div>





</body>

</html>
